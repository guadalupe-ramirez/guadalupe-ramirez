<div align="center">
  <img width="100%" src="https://capsule-render.vercel.app/api?type=waving&height=120&color=0:EE82EE,100:6A0DAD&reversal=true&section=header" />
</div>

<h3 align="center">
  Hola, soy Guadalupe Ram√≠rez P√©rez
  <img src="https://media.giphy.com/media/hvRJCLFzcasrR4ia7z/giphy.gif" width="26">
</h3>

<p align="center">
  <a href="https://github.com/guadalupe-ramirez">
    <img src="https://readme-typing-svg.demolab.com?font=Fira+Code&weight=600&size=18&pause=1200&center=true&vCenter=true&width=600&height=45&lines=Data+Engineer+%7C+Ingenier%C3%ADa+Qu%C3%ADmica;ETL%2FELT%2FETLT+%7C+Cloud+Data+Platforms;Transformando+datos+en+decisiones+accionables" alt="Typing SVG" />
  </a>
</p>

<p align="center">
  <img src="https://komarev.com/ghpvc/?username=guadalupe-ramirez&color=8A2BE2&style=flat" alt="Profile views" />
</p>

---
## Data Engineer con formaci√≥n en Ingenier√≠a Qu√≠mica

Soy **Ingeniera de Datos e Ingeniera Qu√≠mica**, enfocada en transformar datos en soluciones que optimizan procesos, reducen ineficiencias y generan impacto real en el negocio.

---

## Sobre m√≠

<picture>
  <img align="right" src="https://mir-s3-cdn-cf.behance.net/project_modules/disp/601014116770475.6068beff4640a.gif" width="340px">
</picture>

Combino la disciplina de la **Ingenier√≠a Qu√≠mica** con las pr√°cticas de la **Ingenier√≠a de Datos**.

Mi trayectoria comenz√≥ en **manufactura, calidad y log√≠stica**, donde aprend√≠ a valorar:
- la eficiencia en los procesos,
- la precisi√≥n en la informaci√≥n,
- y la importancia de decidir con base en datos.

Esa experiencia industrial gu√≠a hoy mi trabajo con datos: pienso siempre en **sistemas completos**, desde la captura de la informaci√≥n hasta su uso final, y en c√≥mo cada decisi√≥n t√©cnica se refleja en la operaci√≥n.

---

## Mi trabajo como Data Engineer

Mi trabajo se centra en **dise√±ar y construir sistemas de datos** que sean:

- **Eficientes**: evitando procesos manuales y redundancias.  
- **Escalables**: preparados para crecer en volumen y complejidad.  
- **Confiables**: con trazabilidad, calidad y consistencia en los datos.  

En la pr√°ctica, esto significa:

- Construir **pipelines ETL/ELT/ETLT** desde distintas fuentes (APIs, archivos, bases relacionales).
- Dise√±ar modelos de datos claros y consistentes, orientados a responder preguntas de negocio y descubrir insights accionables.
- Implementar procesos de limpieza, validaci√≥n y estandarizaci√≥n que permitan confiar en los datos.
- Documentar y versionar el trabajo para que otros puedan entender, reproducir y mejorar lo que se construye.

Me interesa que las soluciones no solo ‚Äúfuncionen‚Äù, sino que **faciliten el trabajo de analistas, cient√≠ficos de datos y equipos de negocio**.

---

## Lo que quiero construir

M√°s que mover datos de un punto A a un punto B, me motiva **dise√±ar arquitecturas de datos que se conviertan en una ventaja real para las organizaciones**.

Me interesa participar en entornos donde:

- Los datos se traten como un **producto**: con calidad, mantenimiento y evoluci√≥n continua.
- Las decisiones t√©cnicas se tomen entendiendo el **contexto operativo** (planta, log√≠stica, finanzas, etc.).
- La ingenier√≠a de datos sea un **puente** entre la operaci√≥n diaria y la estrategia del negocio.

Mi meta es seguir creciendo en proyectos donde pueda **combinar experiencia industrial, buenas pr√°cticas de ingenier√≠a y tecnolog√≠as modernas de datos** para construir plataformas s√≥lidas y sostenibles.

---

## Tecnolog√≠as y herramientas

**Lenguajes**

- Python (`pandas`, `PySpark`, `requests`, visualizaci√≥n y automatizaci√≥n de scripts)  
- SQL (consultas anal√≠ticas, modelado y creaci√≥n de vistas/tablas en PostgreSQL, MySQL y SQLite)  
- Bash (scripts para automatizar tareas y orquestar procesos)  
- JSON / YAML (configuraci√≥n de pipelines, DBT e infraestructura como c√≥digo)

**Procesamiento y modelado de datos**

- Dise√±o de pipelines **ETL/ELT/ETLT**  
- Modelado relacional y dimensional.  
- Uso de formatos de almacenamiento optimizados (por ejemplo, Parquet) y estrategias de particionamiento por fecha o llaves de negocio.  
- Apache **Spark / PySpark** para transformaciones, joins y agregaciones sobre vol√∫menes crecientes de datos.  
- Trabajo en **Jupyter Notebooks** y **Google Colab** para exploraci√≥n, validaci√≥n y prototipado de transformaciones.    

**Cloud & arquitectura de datos**

- Arquitectura de datos  
  - Dise√±o de **plataformas de datos en la nube** (data lakes, data warehouses y enfoques tipo lakehouse), definiendo zonas l√≥gicas de datos (raw / staging / curated).  
  - Implementaci√≥n de **arquitecturas por capas y orientadas a dominio** (por ejemplo, esquemas Bronze / Silver / Gold, data marts de negocio y vistas anal√≠ticas espec√≠ficas).  

- **AWS**  
  - **S3** (almacenamiento de datos crudos y procesados)  
  - **Glue Data Catalog** (metadatos y tablas)  
  - **EC2** (entornos de procesamiento, por ejemplo para Spark y orquestaci√≥n)  
  - **IAM** (gesti√≥n de roles y pol√≠ticas de acceso con m√≠nimo privilegio)  

- **GCP**  
  - **Cloud Storage** (almacenamiento de archivos y datasets)  
  - **BigQuery** (anal√≠tica y modelo de datos tipo Data Warehouse)  
  - **Compute Engine** (entornos de c√≥mputo para procesamiento de datos)  
  - **IAM** (gesti√≥n de identidades y permisos a nivel de proyecto/recursos)  

**Orquestaci√≥n y transformaci√≥n**

- **Airflow** para orquestaci√≥n de pipelines  
- **DBT** para modelos y transformaciones declarativas  
- Consumo de **APIs REST** para ingesta de datos  

**Infraestructura y desarrollo**

- **Docker** para entornos reproducibles  
- **Terraform** (Infrastructure as Code) en proyectos formativos  
- **Git y GitHub** (ramas, pull requests, trabajo colaborativo)  

**Visualizaci√≥n & an√°lisis**

- **Power BI** para dashboards y an√°lisis orientados a negocio  
- **Streamlit** para prototipos de aplicaciones de datos  
- Visualizaci√≥n en Python:
  - `matplotlib`  
  - `seaborn`  
  - `plotly`  
- Exploraci√≥n interactiva de datos en **Jupyter / Google Colab**  

**Forma de trabajo**

- Documentaci√≥n t√©cnica y funcional orientada a distintos perfiles (t√©cnicos y no t√©cnicos)  
- Trabajo con metodolog√≠as √°giles (**Scrum / Kanban**)  
- Enfoque en **trazabilidad, calidad de datos y reproducibilidad**



## About me
I combine the discipline of **Chemical Engineering** with **Data Engineering** best practices.

My journey started in **manufacturing, quality, and logistics**, where I learned to value:

- process efficiency,
- information accuracy,
- and the importance of data-driven decisions.

That industrial experience shapes how I work with data today: I think in end-to-end systems‚Äîfrom data capture to final consumption‚Äîand how every technical decision impacts real operations.

---

## My work as a Data Engineer

My work focuses on designing and building data systems that are:

- **Efficient:** reducing manual work and eliminating redundancies.
- **Scalable:** ready to grow in volume and complexity.
- **Reliable:** with traceability, data quality, and consistency.

In practice, that means:

- Building **ETL/ELT/ETLT** pipelines from multiple sources (APIs, files, relational databases).
- Designing clear, consistent **data models** to answer business questions and uncover actionable insights.
- Implementing **cleaning, validation, and standardization** processes to build trust in the data.
- Documenting and versioning work so others can understand, reproduce, and improve what‚Äôs built.

I care about solutions that don‚Äôt just ‚Äúwork,‚Äù but genuinely make life easier for analysts, data scientists, and business teams.

---

## What I want to build

More than moving data from point A to point B, I‚Äôm motivated by designing data architectures that become a real competitive advantage for organizations.

I‚Äôm especially interested in environments where:

- data is treated as a **product** (quality, maintenance, continuous evolution),
- technical decisions are made with the **operational context** in mind (plant, logistics, finance, etc.),
- data engineering bridges **day-to-day operations** and **business strategy**.

My goal is to keep growing through projects where I can combine industrial experience, strong engineering practices, and modern data technologies to build solid, sustainable platforms.

---

## Technologies and tools

**Languages**

- **Python** (pandas, PySpark, requests, visualization, scripting automation)
- **SQL** (analytical queries, modeling, creating views/tables in PostgreSQL, MySQL, SQLite)
- **Bash** (automation scripts, process orchestration)
- **JSON / YAML** (pipeline configuration, dbt, infrastructure as code)

**Data processing & modeling**

- **ETL/ELT/ETLT** pipeline design
- **Relational and dimensional modeling**
- Optimized storage formats (e.g., **Parquet**) and partitioning strategies (by date or business keys)
- **Apache Spark / PySpark** for transformations, joins, and aggregations at increasing scale
- **Jupyter Notebooks** and **Google Colab** for exploration, validation, and transformation prototyping

**Cloud & data architecture**

- Cloud data platforms (data lakes, data warehouses, **lakehouse** approaches) with logical zones (raw / staging / curated)
- Layered, domain-oriented architectures (e.g., **Bronze / Silver / Gold**, business data marts, and analytics-ready views)

**AWS**

- **S3** (raw and processed storage)
- **Glue Data Catalog** (metadata and tables)
- **EC2** (compute environments for Spark and orchestration)
- **IAM** (least-privilege roles and policies)

**GCP**

- **Cloud Storage** (files and datasets)
- **BigQuery** (analytics and data warehouse modeling)
- **Compute Engine** (compute for data processing)
- **IAM** (identity and permission management)

**Orchestration & transformation**:

- **Airflow** for pipeline orchestration
- **dbt** for declarative transformations and modeling
- **REST APIs** ingestion

**Infrastructure & development**

- **Docker** for reproducible environments
- **Terraform** (Infrastructure as Code) in learning projects
- **Git & GitHub** (branches, pull requests, collaborative workflows)

**Visualization & analysis**

- **Power BI** for dashboards and business-oriented analytics
- **Streamlit** for data app prototypes
- Python visualization: **matplotlib**, **seaborn**, **plotly**
- Interactive exploration in **Jupyter / Google Colab**

**Working style**

- Technical and functional documentation for both technical and non-technical audiences
- Agile collaboration (Scrum / Kanban)
- Strong focus on **traceability, data quality, and reproducibility**

---

## Contacto/ Contact

Si quieres hablar sobre oportunidades, proyectos o simplemente conectar:
If you‚Äôd like to talk about opportunities, projects, or simply connect:

- üíº **LinkedIn:** *www.linkedin.com/in/guadalupe-ramirez-perez*  
- üìß **Email:** `g.ram.perez2702@gmail.com`  
- üêô **GitHub:** [guadalupe-ramirez](https://github.com/guadalupe-ramirez)
